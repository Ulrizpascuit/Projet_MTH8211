---
title: "Phase 2 du projet"
subtitle: "MTH8211"
author:
  - name: Ulrich Baron-Fournier, Petru Lepreanu
    email: ulrich.baron-fournier@polymtl.ca, petru.lepreanu@polymtl.ca
    affiliation:
      - name: Polytechnique Montr√©al
format:
  pdf:
    keep-tex: false
    documentclass: article
    include-in-header:
      - text: |
            \usepackage{xspace}
            \usepackage[francais]{babel}
    geometry:
      - margin=1in
    papersize: letter
    colorlinks: true
    urlcolor: blue
engine: julia
---

# Remise 2  LSRN : Une m√©thode parall√®le pour les syst√®mes lin√©aires fortement rectangulaires 

### Lien du Github: https://github.com/Ulrizpascuit/Projet_MTH8211.git

## Description de la probl√©matique (r√©vis√©e)

Le projet s‚Äôint√©resse √† la r√©solution de grands syst√®mes lin√©aires fortement rectangulaires, c‚Äôest-√†-dire des probl√®mes aux moindres carr√©s lin√©aires o√π la matrice $A$ est de taille $m\times n$ avec un √©cart extr√™me entre $m$ et $n$ (beaucoup plus d‚Äô√©quations que d‚Äôinconnues si $m \gg n$, cas surd√©termin√©, ou l‚Äôinverse $m \ll n$, cas sous-d√©termin√©). Dans de telles situations, on cherche typiquement √† calculer la solution de norme minimale du probl√®me
\[
\min_x \frac{1}{2}\|A x - b\|_2^2,
\]
c‚Äôest-√†-dire la solution de plus petite longueur qui satisfait au mieux $Ax \approx b$. Ces probl√®mes apparaissent dans de nombreuses applications scientifiques et d‚Äôing√©nierie, et la demande en solveurs plus rapides et pr√©cis s‚Äôaccro√Æt avec la taille des donn√©es et des mod√®les. Les approches classiques pour les moindres carr√©s (telles que la r√©solution des √©quations normales $A^T A x = A^T b$ ou les factorisations QR/SVD) deviennent co√ªteuses ou peu pratiques lorsque $m$ et $n$ sont tr√®s d√©s√©quilibr√©s, et peuvent en outre souffrir de probl√®mes de conditionnement num√©rique.

Les m√©thodes it√©ratives comme LSQR r√©duisent les besoins en m√©moire, mais leur vitesse de convergence d√©pend fortement du conditionnement de $A^T A$. Dans le cas de syst√®mes extr√™mement rectangulaires, il est important de pr√©conditionner le probl√®me pour en am√©liorer le conditionnement avant d‚Äôappliquer un solveur it√©ratif. C‚Äôest dans ce contexte qu‚Äôintervient LSRN, une m√©thode introduisant un √©l√©ment al√©atoire dans le processus de r√©solution. LSRN utilise un √©chantillonnage gaussien al√©atoire (projection al√©atoire) pour construire un pr√©conditionneur qui rend le syst√®me pr√©conditionn√© extr√™mement bien conditionn√© avec une probabilit√© √©lev√©e. De plus, cette √©tape de pr√©conditionnement est hautement parall√©lisable et profite de la structure creuse des matrices ou d‚Äôop√©rateurs lin√©aires rapides, ce qui la rend adapt√©e au calcul distribu√© moderne.

En somme, l‚Äôobjectif est de r√©soudre rapidement et avec pr√©cision des syst√®mes tr√®s rectangulaires de grande taille ‚Äì un d√©fi que des m√©thodes al√©atoires parall√®les r√©centes comme LSRN cherchent √† relever. Dans le cadre de ce projet, nous avons pour ambition de lire et comprendre en d√©tail l‚Äôalgorithme LSRN (d√©crit par Meng et al. en 2014), puis d‚Äôen fournir une impl√©mentation efficace en Julia et d‚Äô√©valuer ses performances sur des probl√®mes de grande dimension. Le pr√©sent rapport (Phase¬†2) fait suite √† une phase¬†1 o√π la probl√©matique et les objectifs ont √©t√© d√©finis, et il se concentre sur l‚Äôimpl√©mentation de LSRN (version s√©quentielle et parall√®le) ainsi que sur les r√©sultats pr√©liminaires obtenus. Le code d√©velopp√© est disponible dans le d√©p√¥t GitHub du projet ([lien GitHub √† ins√©rer]).

## Impl√©mentation de la m√©thode LSRN en Julia

### Version s√©quentielle (implantation de base)

La version s√©quentielle de LSRN a √©t√© cod√©e comme une preuve de concept initiale, en suivant pas √† pas l‚Äôalgorithme d√©crit dans l‚Äôarticle de Meng et al. (2014). Pour un syst√®me $A x \approx b$ de dimensions $m\times n$, l‚Äôalgorithme s‚Äôex√©cute de la fa√ßon suivante‚ÄØ:

**√âtape¬†1‚ÄØ: Projection al√©atoire.**  
On g√©n√®re une matrice al√©atoire $G$ de dimensions appropri√©es (de taille $l \times m$ si $m \ge n$, ou $l \times n$ dans le cas sous-d√©termin√©) avec des entr√©es i.i.d. selon $\mathcal{N}(0,1)$. Le param√®tre $l$ (nombre de lignes √©chantillonn√©es) est choisi un peu plus grand que le minimum ($n$ ou $m$) afin de garantir avec une probabilit√© √©lev√©e que $\mathrm{rang}(G A) = \mathrm{rang}(A)$. Par exemple, nous utilisons typiquement $l = \lceil 2n \rceil$ dans le cas surd√©termin√©, conform√©ment aux recommandations de l‚Äôarticle original. Ensuite, on calcule la matrice projet√©e $B = G A$, de taille $l \times n$.

**√âtape¬†2‚ÄØ: Pr√©conditionneur via factorisation.**  
On factorise la matrice $B$ pour en extraire un pr√©conditionneur efficace. Dans notre impl√©mentation s√©quentielle, nous employons une d√©composition QR sur $B$. Plus pr√©cis√©ment, on calcule la factorisation $B = Q R$ (QR ¬´¬†√©conomie¬†¬ª, avec $Q \in \mathbb{R}^{l\times n}$ et $R \in \mathbb{R}^{n\times n}$ triangulaire sup√©rieure). La matrice $R$ obtenue sert de pr√©conditionneur pour le syst√®me original. Intuitivement, $R$ approxime (avec de bonnes garanties probabilistes) un facteur de Cholesky de $A^T A$ bien conditionn√©.

**√âtape¬†3‚ÄØ: R√©solution it√©rative pr√©conditionn√©e.**  
Une fois $R$ calcul√©, nous devons r√©soudre le syst√®me $Ax \approx b$ en utilisant $R$ comme pr√©conditionneur √† gauche. Concr√®tement, nous voulons r√©soudre $ \min_x \|A x - b\|_2$ avec $R^{-1}A$ au lieu de $A$ (et $R^{-1}b$ au lieu de $b$) afin d‚Äôacc√©l√©rer la convergence. Nous faisons appel √† un solveur de Krylov adapt√© aux moindres carr√©s, en l‚Äôoccurrence l‚Äôalgorithme LSQR (fourni par la biblioth√®que Krylov.jl).

L‚Äôimpl√©mentation s√©quentielle a √©t√© valid√©e sur des cas tests de petite taille afin de s‚Äôassurer du bon fonctionnement de chaque √©tape. Par exemple, nous avons r√©solu un petit syst√®me dense al√©atoire et compar√© la solution obtenue avec celle donn√©e par un solveur direct (QR de Julia) pour v√©rifier que les solutions co√Øncident √† la pr√©cision pr√®s. De m√™me, sur un exemple creux al√©atoire, nous avons confirm√© que l‚Äôalgorithme renvoie une r√©siduelle tr√®s faible.

### Version parall√®le (multi‚Äêthreading en Julia)

La m√©thode LSRN se pr√™te particuli√®rement bien √† la parall√©lisation, en raison notamment de son √©tape de projection al√©atoire qui peut exploiter des calculs matriciels de grande taille hautement parall√©lisables. Nous avons donc d√©velopp√© en parall√®le une version multi-thread de l‚Äôimpl√©mentation Julia, dans le but de r√©duire les temps de calcul sur les grandes instances.

**G√©n√©ration et multiplication al√©atoire en parall√®le**  
La g√©n√©ration de la matrice $G$ et le calcul de $B = G A$ sont r√©partis entre plusieurs threads Julia. Chaque thread multiplie une sous-matrice de $G$ par $A$, puis les r√©sultats sont combin√©s pour former $B$. Cette parall√©lisation par blocs est d‚Äôautant plus efficace que $A$ est de grande taille.

**Factorisation QR multi-thread√©e**  
Une fois la matrice $B$ assembl√©e, nous utilisons les routines de factorisation QR optimis√©es de Julia (bas√©es sur LAPACK/BLAS pour le dense, ou SPQR pour le creux). Ces librairies exploitent le parall√©lisme interne pour acc√©l√©rer le calcul de $R$.

**Phase it√©rative parall√®le**  
L‚Äôalgorithme LSQR en lui-m√™me est essentiellement s√©quentiel, mais chaque produit matrice-vecteur par $A$ ou $A^T$ b√©n√©ficie du parall√©lisme des op√©rations BLAS ou du multi-threading sur matrices creuses. Le gain de parall√©lisation est surtout visible pour les grandes matrices denses.

En r√©sum√©, la version parall√®le de LSRN exploite le multi-threading lors des √©tapes les plus co√ªteuses (g√©n√©ration/projection al√©atoire, multiplications matrice-vecteur), ce qui permet d‚Äôacc√©l√©rer significativement la r√©solution sur des machines multi-c≈ìurs. Cet aspect n'a pas encore √©t√© ajout√© dans le code et donc aucune analyse et comparaison avec les algorithmes s√©quentiel n'ont √©t√© faites.

## R√©sultats num√©riques pr√©liminaires

### Matrices denses al√©atoires

Pour √©valuer les performances, nous avons g√©n√©r√© des matrices dense de taille $m = 10\,000, n = 1\,000$ et $m = 100\,000, n = 10\,00$(sur-d√©termin√©e, $m \gg n$). Les entr√©es de $A$ ont √©t√© tir√©es uniform√©ment sur $[-1,1]$ et le vecteur $b$ √©galement al√©atoire. Nous comparons l‚Äôex√©cution de l'algorithme LSQR de base √† l'algorithme LSQR en utilisant le pr√©conditionnement de LSRN en version s√©quentielle. Les r√©sultats sont r√©sum√©s ci-dessous‚ÄØ:

| Syst√®me (dense)       | It√©ration LSQR      | It√©rations LSRN_LSQR | Residu relatif LSQR | Residu relatif LSRN_LSQR |
|:----------------------|:-------------------:|:--------------------:|:-------------------:|:-------------------:|
| $A$ $10^4\times10^3$  |       1498          |           43         | $6.44\times10^{-7}$ | $1.78\times10^{-7}$ |
| $A$ $10^5\times10^3$  |       1501          |           43         | $3.43\times10^{-7}$ | $1.78\times10^{-7}$ |

Dans le premier cas ($m=10\,000, n=1\,000$), LSQR trouve la solution en 1498 it√©rations, avec un r√©sidu relatif $|Ax-b|/|b|$ d'environ $6.44\times10^{-7}$ et LSRN_LSQR trouve la solution en 43 it√©rations avec un r√©sidu relatif d'environ $1.78\times10^{-7}$ (plus faible que LSQR). On voit donc que l'application de LSRN permet √† LSQR de r√©soudre le probl√®me en beaucoup moins d'it√©rations pour obtenir un r√©sidu qui est m√™me plus faible. Dans le deuxi√®me cas, On trouve les m√™mes genres de r√©sultats, ce qui confirme le bon fonctionnement de l'algorithme LSRN!

De plus, un benchmark a √©t√© effectu√© pour la m√©thode LSQR et la m√©thode LSRN_LSQR afin de comparer les temps de calcul pour les m√™me deux probl√®mes. 

| Syst√®me (dense)       | Temps moyen LSQR        | Temps moyen LSRN_LSQR   |
|:----------------------|:-----------------------:|:-----------------------:|
| $A$ $10^4\times10^3$  | 2.604 s    ¬± 81.057 ms  | 900.802 ms ¬±  34.475 ms |
| $A$ $10^5\times10^3$  | 34.445 s                | 4.110 s ¬± 286.480 ms    |

On peut voir ici que le temps de calcul pour la m√©thode LSRN_LSQR est beaucoup plus faible que pour LSQR. Cela montre donc que le pr√©conditionneur de LSRN permet vraiment de rendre le probl√®me plus facile √† r√©soudre en am√©liorement son conditionnement. On voit la puissance que peuvent avoir les pr√©conditionneurs bas√©s sur l'al√©atoire.


### Matrices creuses (sparse) al√©atoires et r√©elles
Encore aucun test a √©t√© fait pour les matrices creuses. Le but serait le m√™me que pour les matrices dense. Il faut comparer des solveurs de probl√®mes creux avant et apr√®s l'application du pr√©conditionneur provenant de LSRN. 

## Subtilit√©s et d√©fis rencontr√©s en phase¬†2

### Choix initial du solveur
Dans l'article, pluisieurs solveurs ont √©t√© utilis√©s et compar√© entre eux.

### Cas sous-d√©termin√©
LSRN a √©t√© adapt√© pour $m < n$ en appliquant la projection al√©atoire sur $A^T$.

### Utilisation d'exemples test multiples‚ÄØ
Afin d'avoir plus de flexibilit√© sur les probl√®mes fortement rectangulaires, Un fonction g√©n√©rative de matrice a √©t√©

### Gestion du parall√©lisme‚ÄØ
En premier lieu, nous avons essay√© de produire un algorithme qui utilise la parall√©lisme avec l'astuce suivante:
```julia
 nthreads = Threads.nthreads()
    blocksize = ceil(Int, s / nthreads)

    Threads.@threads for t = 1:nthreads
        i1 = (t-1)*blocksize + 1
        i2 = min(t*blocksize, s)
        if i1 <= i2 
            B[i1:i2, :] .= G[i1:i2, :] * A
        end
    end
```
En utilisant cet astuce pour utiliser le parall√©lisme, nous n'avons pas remarqu√© de gain de performance. Cela peut √™tre d√ª √† √©normemant de chose. Cependant, la librairie BLAS effectue du calcul multi-thread√© de base. 
**Tests de robustesse**‚ÄØ: Les cas mal conditionn√©s et de rang d√©ficient sont bien g√©r√©s gr√¢ce au pr√©conditionneur al√©atoire.

## √âch√©ancier r√©capitulatif et prochaines √©tapes

| √âtape                                                                       |   Statut    |
|:----------------------------------------------------------------------------|:------------|
| Reproduction de l'algorithme LSRN de base pour syst√®me sur d√©termin√©        | R√©alis√©e ‚úÖ-|
| Reproduction de l'algorithme LSRN de base pour syst√®me sous d√©termin√©       | R√©alis√©e ‚úÖ-|
| Tests pr√©liminaires sur matrices al√©atoires.                                | R√©alis√©e ‚úÖ-|
| R√©daction du rapport interm√©diaire (phase 2).                               | R√©alis√©e ‚úÖ |
| Tests √† grande √©chelle, comparaison avec DGELSD, SuiteSparseQR, Blendenpik? | √Ä faire üî≤ |
| Validation de la m√©thode parall√®le                                          | √Ä faire üî≤ |
| Optimisations finales du code, documentation                                | √Ä faire üî≤ |
| R√©daction du rapport final (phase 3)                                        | √Ä faire üî≤ |

## Conclusion provisoire

√Ä l‚Äôissue de cette phase¬†2, nous disposons d‚Äôune impl√©mentation de base fonctionnelle de LSRN en Julia, capable de r√©soudre des syst√®mes lin√©aires fortement rectangulaires de grande taille avec une haute pr√©cision. Les premiers r√©sultats obtenus sont encourageants‚ÄØ: ils confirment les performances attendues de LSRN en termes de nombre d'it√©ration et de temps. Dans la prochaine phase, nous poursuivrons les tests √† grande √©chelle et les comparaisons approfondies avec d‚Äôautres solveurs. Le projet permet d‚Äôillustrer concr√®tement l‚Äôapport des m√©thodes al√©atoires pour l‚Äôalg√®bre lin√©aire num√©rique √† grande √©chelle.

---


