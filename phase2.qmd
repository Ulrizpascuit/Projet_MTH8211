---
title: "Phase 2 du projet"
subtitle: "MTH8211"
author:
  - name: Ulrich Baron-Fournier
    email: ulrich.baron-fournier@polymtl.ca
    affiliation:
      - name: Polytechnique Montr√©al
format:
  pdf:
    keep-tex: false
    documentclass: article
    include-in-header:
      - text: |
            \usepackage{xspace}
            \usepackage[francais]{babel}
    geometry:
      - margin=1in
    papersize: letter
    colorlinks: true
    urlcolor: blue
engine: julia
---

# Remise 2  LSRN : Une m√©thode parall√®le pour les syst√®mes lin√©aires fortement rectangulaires 

### Lien du Github: https://github.com/Ulrizpascuit/Projet_MTH8211.git

## Description de la probl√©matique (r√©vis√©e)

Le projet s‚Äôint√©resse √† la r√©solution de grands syst√®mes lin√©aires fortement rectangulaires, c‚Äôest-√†-dire des probl√®mes aux moindres carr√©s lin√©aires o√π la matrice $A$ est de taille $m\times n$ avec un √©cart extr√™me entre $m$ et $n$ (beaucoup plus d‚Äô√©quations que d‚Äôinconnues si $m \gg n$, cas surd√©termin√©, ou l‚Äôinverse $m \ll n$, cas sous-d√©termin√©). Dans de telles situations, on cherche typiquement √† calculer la solution de norme minimale du probl√®me 
$\min_x \frac{1}{2}||A x - b||_2^2$
c‚Äôest-√†-dire la solution de plus petite longueur qui satisfait au mieux $Ax \approx b$. Ces probl√®mes apparaissent dans de nombreuses applications scientifiques et d‚Äôing√©nierie, et la demande en solveurs plus rapides et pr√©cis s‚Äôaccro√Æt avec la taille des donn√©es et des mod√®les. Les approches classiques pour les moindres carr√©s (telles que la r√©solution des √©quations normales $A^T A x = A^T b$ ou les factorisations QR/SVD) deviennent co√ªteuses ou peu pratiques lorsque $m$ et $n$ sont tr√®s d√©s√©quilibr√©s, et peuvent en outre souffrir de probl√®mes de conditionnement num√©rique.

Les m√©thodes it√©ratives comme LSQR r√©duisent les besoins en m√©moire, mais leur vitesse de convergence d√©pend fortement du conditionnement de $A^T A$. Dans le cas de syst√®mes extr√™mement rectangulaires, il est important de pr√©conditionner le probl√®me pour en am√©liorer le conditionnement avant d‚Äôappliquer un solveur it√©ratif. C‚Äôest dans ce contexte qu‚Äôintervient LSRN, une m√©thode introduisant un √©l√©ment al√©atoire dans le processus de r√©solution. LSRN utilise un √©chantillonnage gaussien al√©atoire (projection al√©atoire) pour construire un pr√©conditionneur qui rend le syst√®me pr√©conditionn√© extr√™mement bien conditionn√© avec une probabilit√© √©lev√©e. De plus, cette √©tape de pr√©conditionnement est hautement parall√©lisable et profite de la structure creuse des matrices ou d‚Äôop√©rateurs lin√©aires rapides, ce qui la rend adapt√©e au calcul distribu√© moderne.

En somme, l‚Äôobjectif est de r√©soudre rapidement et avec pr√©cision des syst√®mes tr√®s rectangulaires de grande taille ‚Äì un d√©fi que des m√©thodes al√©atoires parall√®les r√©centes comme LSRN cherchent √† relever. Dans le cadre de ce projet, nous avons pour ambition de lire et comprendre en d√©tail l‚Äôalgorithme LSRN (d√©crit par Meng et al. en 2014), puis d‚Äôen fournir une impl√©mentation efficace en Julia et d‚Äô√©valuer ses performances sur des probl√®mes de grande dimension. Le pr√©sent rapport (Phase¬†2) fait suite √† une phase¬†1 o√π la probl√©matique et les objectifs ont √©t√© d√©finis, et il se concentre sur l‚Äôimpl√©mentation de LSRN (version s√©quentielle et parall√®le) ainsi que sur les r√©sultats pr√©liminaires obtenus. Le code d√©velopp√© est disponible dans le d√©p√¥t GitHub du projet (https://github.com/Ulrizpascuit/Projet_MTH8211.git).

## Impl√©mentation de la m√©thode LSRN en Julia

### Version s√©quentielle (implantation de base)

La version s√©quentielle de LSRN a √©t√© cod√©e comme une preuve de concept initiale, en suivant pas √† pas l‚Äôalgorithme d√©crit dans l‚Äôarticle de Meng et al. (2014). Pour un syst√®me $A x \approx b$ de dimensions $m\times n$, l‚Äôalgorithme s‚Äôex√©cute de la fa√ßon suivante‚ÄØ:

**√âtape¬†1‚ÄØ: Projection al√©atoire.**  
On g√©n√®re une matrice al√©atoire $G$ de dimensions appropri√©es (de taille $s \times m$ si $m \ge n$, ou $s \times n$ dans le cas sous-d√©termin√©) avec des entr√©es i.i.d. selon $\mathcal{N}(0,1)$. Le param√®tre $s$ (nombre de lignes √©chantillonn√©es) est choisi un peu plus grand que le minimum ($n$ ou $m$) afin de garantir avec une probabilit√© √©lev√©e que $\mathrm{rang}(G A) = \mathrm{rang}(A)$. Par exemple, nous utilisons typiquement $s = \lceil 2n \rceil$ dans le cas surd√©termin√©, conform√©ment aux recommandations de l‚Äôarticle original. Ensuite, on calcule la matrice projet√©e $\tilde{A} = G A$, de taille $s \times n$.

**√âtape¬†2‚ÄØ: Pr√©conditionneur via factorisation.**  
On factorise la matrice $\tilde{A}$ pour en extraire un pr√©conditionneur efficace. Dans notre impl√©mentation s√©quentielle, nous employons une d√©composition SVD sur $\tilde{A}$. Plus pr√©cis√©ment, on calcule la factorisation $\tilde{A} = \tilde{U}\tilde{\Sigma}\tilde{V}^T$(avec $\tilde{U} \in \mathbb{R}^{s\times n}$, $\tilde{\Sigma}\in \mathbb{R}^{r\times r}$ et $\tilde{A}\in \mathbb{R}^{n\times r}$). En multipliant $\tilde{V}$ par l'inverse de $\tilde{\Sigma}$, on obtient $N = \tilde{V} \tilde{\Sigma}^{-1}$ le pr√©conditionneur pour le syst√®me original.

**√âtape¬†3‚ÄØ: R√©solution it√©rative pr√©conditionn√©e.**  
Une fois $N$ calcul√©, nous devons r√©soudre le syst√®me $Ax \approx b$ en utilisant $N$ comme pr√©conditionneur √† gauche. Concr√®tement, nous voulons r√©soudre $\min_x \frac{1}{2}||AN x - b||_2^2$ avec la solution finale √©tant $\hat{x} = N\hat{y}$ afin d‚Äôacc√©l√©rer la convergence. Nous faisons appel √† un solveur de Krylov adapt√© aux moindres carr√©s, en l‚Äôoccurrence l‚Äôalgorithme LSQR (fourni par la biblioth√®que Krylov.jl).

L‚Äôimpl√©mentation s√©quentielle a √©t√© valid√©e sur des cas tests de petite taille afin de s‚Äôassurer du bon fonctionnement de chaque √©tape. Par exemple, nous avons r√©solu un petit syst√®me dense al√©atoire et compar√© la solution obtenue avec celle donn√©e par un solveur direct (QR de Julia) pour v√©rifier que les solutions co√Øncident √† la pr√©cision pr√®s. L'impl√©mentation est la suivante:

```julia
function lsrn_lsqr(A, b; gamma=2.0, tol=1e-10, itmax=2000)
    m, n = size(A)
    s = ceil(Int, gamma * n) 
    G = randn(s, m)
    AÃÉ = G * A
    UÃÉ, Œ£ÃÉ, VÃÉ = svd(AÃÉ; full=false)
    r = sum(Œ£ÃÉ.> 1e-12)
    Œ£inv = Diagonal(1.0 ./ Œ£ÃÉ[1:r])
    V_r = VÃÉ[:,1:r]
    N = V_r * Œ£inv
    AN = A * N
    yÃÇ, histo = lsqr(AN, b; atol=tol, btol=tol, itmax=itmax, history=true)
    xÃÇ = N * yÃÇ
    return xÃÇ, histo
end
```


### Version parall√®le (multi‚Äêthreading en Julia)

La m√©thode LSRN se pr√™te particuli√®rement bien √† la parall√©lisation, en raison notamment de son √©tape de projection al√©atoire qui peut exploiter des calculs matriciels de grande taille hautement parall√©lisables. Nous avons donc d√©velopp√© en parall√®le une version multi-thread de l‚Äôimpl√©mentation Julia, dans le but de r√©duire les temps de calcul sur les grandes instances.

**G√©n√©ration et multiplication al√©atoire en parall√®le**  
La g√©n√©ration de la matrice $G$ et le calcul de $\tilde{A} = G A$ sont r√©partis entre plusieurs threads Julia. Chaque thread multiplie une sous-matrice de $G$ par $A$, puis les r√©sultats sont combin√©s pour former $\tilde{A}$. Cette parall√©lisation par blocs est d‚Äôautant plus efficace que $A$ est de grande taille.

**Factorisation QR multi-thread√©e**  
Une fois la matrice $\tilde{A}$ assembl√©e, nous utilisons les routines de factorisation QR optimis√©es de Julia (bas√©es sur LAPACK/BLAS pour le dense, ou SPQR pour le creux). Ces librairies exploitent le parall√©lisme interne pour acc√©l√©rer le calcul de $N$.

**Phase it√©rative parall√®le**  
L‚Äôalgorithme LSQR en lui-m√™me est essentiellement s√©quentiel, mais chaque produit matrice-vecteur par $A$ ou $A^T$ b√©n√©ficie du parall√©lisme des op√©rations BLAS ou du multi-threading sur matrices creuses. Le gain de parall√©lisation est surtout visible pour les grandes matrices denses.

En r√©sum√©, la version parall√®le de LSRN exploite le multi-threading lors des √©tapes les plus co√ªteuses (g√©n√©ration/projection al√©atoire, multiplications matrice-vecteur), ce qui permet d‚Äôacc√©l√©rer significativement la r√©solution sur des machines multi-c≈ìurs. Cet aspect n'a pas encore bien √©t√© analys√© pour des raisons techniques qui seront pr√©sent√©es dans la section: Subtilit√©s / d√©fis rencontr√©s.

## R√©sultats num√©riques pr√©liminaires
### Cr√©ation de matrice test et conditionnement
Afin d'effectuer les test n√©cessaire pour montrer la pertinence de LSRN dans le cadre de r√©solution de syst√®me fortement rectangulaire et mal conditionn√©, des fonction permettant de g√©n√©rer des matrice sur-d√©termin√©es et sous-d√©termin√©es avec ces caract√©ristiques ont √©t√© produites:

```julia
function badly_conditioned_rectangular_matrix(m, n, kappa)
    U, _ = qr(randn(m, n))
    V, _ = qr(randn(n, n))
    s = range(1.0, 1.0/kappa, length=n)
    S = Diagonal(s)
    A = U * S * V'
    return Matrix(A)
end
```

```julia
function badly_conditioned_underdetermined_matrix(m, n, kappa)
    U, _ = qr(randn(n, m))
    V, _ = qr(randn(m, m))
    s = range(1.0, 1.0 / kappa, length=m)
    S = Diagonal(s)
    A_tall = U * S * V'
    return Matrix(A_tall')[1:m, 1:n]
end
```

En appliquant le pr√©conditionneur de LSRN sur les matrices g√©n√©r√©es par les fonction ci-dessus, on voit une √©norme am√©lioration du conditionnement comme pr√©vu:

| Syst√®me (dense)              | Cond(A)             | Cond(A precond)   |
|:-----------------------------|:-------------------:|:-----------------:|
| dim($A$) = $10^4\times10^3$  |   $1\times10^{9}$   |      5.67         |
| dim($A$) = $10^5\times10^3$  |   $1\times10^{9}$   |      5.69         |

### Matrices denses al√©atoires

*Les r√©sultats de cette section ont √©t√© obtenu avec un nombre de thread √©gal √† 16 (BLAS.set_num_threads(16)) et un conditionnement des matrice √©gal √† 1 milliard (kappa = 1e9).*

Pour √©valuer les performances, nous avons g√©n√©r√© des matrices dense de taille $m = 10\,000, n = 1\,000$ et $m = 100\,000, n = 10\,00$(sur-d√©termin√©e, $m \gg n$). Les entr√©es de $A$ ont √©t√© tir√©es uniform√©ment sur $[-1,1]$ et le vecteur $b$ √©galement al√©atoire. Nous comparons l‚Äôex√©cution de l'algorithme LSQR de base √† l'algorithme LSQR en utilisant le pr√©conditionnement de LSRN en version s√©quentielle. Les r√©sultats sont r√©sum√©s ci-dessous‚ÄØ:

| Syst√®me (dense)       | It√©ration LSQR      | It√©rations LSRN_LSQR | Residu relatif LSQR | Residu relatif LSRN_LSQR |
|:----------------------|:-------------------:|:--------------------:|:-------------------:|:-------------------:|
| dim($A$) = $10^4\times10^3$  |       1498          |           43         | $6.44\times10^{-7}$ | $1.78\times10^{-7}$ |
| dim($A$) = $10^5\times10^3$  |       1501          |           43         | $3.43\times10^{-7}$ | $1.78\times10^{-7}$ |

Dans le premier cas ($m=10\,000, n=1\,000$), LSQR trouve la solution en 1498 it√©rations, avec un r√©sidu relatif $||Ax-b||/||b||$ d'environ $6.44\times10^{-7}$ et LSRN_LSQR trouve la solution en 43 it√©rations avec un r√©sidu relatif d'environ $1.78\times10^{-7}$ (plus faible que LSQR). On voit donc que l'application de LSRN permet √† LSQR de r√©soudre le probl√®me en beaucoup moins d'it√©rations pour obtenir un r√©sidu qui est m√™me plus faible. Dans le deuxi√®me cas, On trouve les m√™mes genres de r√©sultats, ce qui confirme le bon fonctionnement de l'algorithme LSRN!

De plus, un benchmark a √©t√© effectu√© pour la m√©thode LSQR et la m√©thode LSRN_LSQR afin de comparer les temps de calcul pour les m√™me deux probl√®mes. 

| Syst√®me (dense)       | Temps moyen LSQR        | Temps moyen LSRN_LSQR   |
|:----------------------|:-----------------------:|:-----------------------:|
| dim($A$) = $10^4\times10^3$  | 2.604 s    ¬± 81.057 ms  | 900.802 ms ¬±  34.475 ms |
| dim($A$) = $10^5\times10^3$  | 34.445 s                | 4.110 s ¬± 286.480 ms    |

On peut voir ici que le temps de calcul pour la m√©thode LSRN_LSQR est beaucoup plus faible que pour LSQR. Cela montre donc que le pr√©conditionneur de LSRN permet vraiment de rendre le probl√®me plus facile √† r√©soudre en am√©liorement son conditionnement. On voit la puissance que peuvent avoir les pr√©conditionneurs bas√©s sur l'al√©atoire.


### Matrices creuses (sparse) al√©atoires et r√©elles
Encore aucun test a √©t√© fait pour les matrices creuses. Le but serait le m√™me que pour les matrices dense. Il faut comparer des solveurs de probl√®mes creux avant et apr√®s l'application du pr√©conditionneur provenant de LSRN. Cette section sera probablement enlev√©e pour le rapport final si les autres objectifs prennent trop de temps. Dans le cas contraire, nous allons produire des fonctions permettant de g√©n√©rer des matrices creuses de dimensions et conditionnement d√©sir√©s et adapter les algorithmes afin de pouvoir r√©soudres les probl√®mes en exploitant le syst√®me creux.

## Subtilit√©s / d√©fis rencontr√©s en phase¬†2 et prochaines √©tapes

### Choix initial du solveur
Dans l'article, pluisieurs solveurs ont √©t√© utilis√©s et compar√© entre eux. Afin de simplifier le tout, la fonction LSQR de krylov a √©t√© choisi pour pouvoir faire la la distinction entre l'utilisation ou non du pr√©conditionneur LSRN. Cependant, pour le rapport, final il serait int√©ressant de voir l'am√©lioration qu'apporte LSRN en terme d'it√©ration et de temps de r√©solution sur d'autre solveur (LSMR par exemple). Cela pourrait aussi faire amener une discussion sur la robustesse des solveurs en fonction du conditionnement.

### Cas sous-d√©termin√©
LSRN a √©t√© adapt√© pour $m < n$ en appliquant la projection al√©atoire sur $A^T$. Certains r√©sultats pr√©liminaires ont √©t√© produits avec 

### Utilisation d'exemples test multiples‚ÄØ
Afin d'avoir plus de flexibilit√© sur les probl√®mes fortement rectangulaires, des fonctions g√©n√©ratives de matrices ont √©t√© produites afin de couvrir les syst√®mes sur-d√©termin√©s et sous-d√©termin√©s. Cependant, pour le rapport final, nous essairons de trouver des probl√®mes de la litt√©rature qui correspondent aux probl√®mes recherch√©s afin de voir si les gains de LSRN se voient autant sur des matrices moins randomis√©es.

### Gestion du parall√©lisme‚ÄØ
En premier lieu, nous avons essay√© de produire un algorithme qui utilise le parall√©lisme avec l'astuce suivante:
```julia
 nthreads = Threads.nthreads()
    blocksize = ceil(Int, s / nthreads)

    Threads.@threads for t = 1:nthreads
        i1 = (t-1)*blocksize + 1
        i2 = min(t*blocksize, s)
        if i1 <= i2 
            B[i1:i2, :] .= G[i1:i2, :] * A
        end
    end
```
pour essayer de passer des calculs en parall√®le lors de la multiplication de matrice. En utilisant cet astuce pour utiliser le parall√©lisme, nous n'avons pas remarqu√© de gain de performance si on compare le temps de calcul en faisant l'op√©ration de base AÃÉ = G * A. Cela peut √™tre d√ª √† √©normemant de chose. Cependant, la librairie BLAS effectue d√©j√† du calcul multi-thread√© de base. Cela fait en sorte que le parall√©lisme impl√©ment√© (si il fonctionne vraiment), ne peut pas montrer la diff√©rence avec l'op√©ration de base si celle-ci utilise d√©j√† le parall√©lisme. Cela √©tant dit, il est toujours possible comparer l'algorithme avec et sans parall√©lisme en contr√¥lant le nombre de threads que BLAS a le droit d'utiliser. Les fonctions suivantes seront utilis√©es:

```julia
BLAS.set_num_threads(16)
```

pour d√©finir le nombre de threads que BLAS utilise et
 
```julia
BLAS.get_num_threads()
```

pour v√©rifier le nombre de thread qu'il utilise. Ainsi, Il sera possible de tracer la performance du parall√©lisme en fonction de la dimension du syst√®me.

### Tests finaux pour le rapport 3

Premi√®rement, des tests suppl√©mentaires seront r√©alis√©s afin de comparer le nombre d‚Äôit√©rations et le temps d‚Äôex√©cution de LSRN en fonction du conditionnement de la matrice initiale et de ses dimensions. Nous avons en effet observ√© qu‚Äô√† partir d‚Äôun certain seuil (lorsque la matrice devient moins rectangulaire ou que son conditionnement se rapproche de 1), l‚Äôutilisation du pr√©conditionneur de LSRN peut nuire √† la performance globale de l‚Äôalgorithme (pr√©conditionnement + solveur). Cela pourrait indiquer qu‚Äôil existe certaines configurations de dimensions et de conditionnement pour lesquelles LSRN devient inutile, voire contre-productif. Ces r√©sultats pourraient √™tre repr√©sent√©s √† l‚Äôaide d‚Äôun graphique √† deux variables ind√©pendantes afin d‚Äôillustrer le gain apport√© par LSRN en fonction √† la fois du conditionnement et des dimensions de la matrice.

Deuxi√®mement, plusieurs versions de ce graphique pourront √™tre produites en d√©finissant un nombre de threads BLAS diff√©rent pour chaque cas, afin d‚Äôanalyser l‚Äôimpact du parall√©lisme sur l‚Äôalgorithme. Ainsi, cette s√©rie de r√©sultats permettrait d‚Äôidentifier d‚Äô√©ventuels points critiques ou r√©gions de l‚Äôespace des param√®tres qui sugg√©reraient l‚Äôutilisation de LSRN.

Enfin, tous les tests r√©alis√©s sur les probl√®mes surd√©termin√©s seront reproduits de la m√™me mani√®re sur des probl√®mes sous-d√©termin√©s, afin de valider le comportement de LSRN sur toute matrice fortement rectangulaire.

## √âch√©ancier r√©capitulatif et prochaines √©tapes

| √âtape                                                                       |   Statut    |
|:----------------------------------------------------------------------------|:------------|
| Reproduction de l'algorithme LSRN de base pour syst√®me sur d√©termin√©        | R√©alis√©e ‚úÖ-|
| Reproduction de l'algorithme LSRN de base pour syst√®me sous d√©termin√©       | R√©alis√©e ‚úÖ-|
| Tests pr√©liminaires sur matrices al√©atoires.                                | R√©alis√©e ‚úÖ-|
| R√©daction du rapport interm√©diaire (phase 2).                               | R√©alis√©e ‚úÖ |
| Validation du bon fonctionnement de LSRN pour des matrices sous d√©termin√©es | √Ä faire üî≤ |
| Efficacit√© de LSRN en fonction du conditionnement et de la taille           | √Ä faire üî≤ |
| Validation de l'impact du multi-threading de BLAS                           | √Ä faire üî≤ |
| Optimisations finales du code, documentation                                | √Ä faire üî≤ |
| R√©daction du rapport final (phase 3)                                        | √Ä faire üî≤ |

## Conclusion provisoire

√Ä l‚Äôissue de cette phase¬†2, nous disposons d‚Äôune impl√©mentation de base fonctionnelle de LSRN en Julia, capable de r√©soudre des syst√®mes lin√©aires fortement rectangulaires de grande taille avec une haute pr√©cision. Les premiers r√©sultats obtenus sont encourageants‚ÄØ: ils confirment les performances attendues de LSRN en termes de nombre d'it√©ration et de temps. Dans la prochaine phase, nous poursuivrons les tests √† grande √©chelle, des comparaisons approfondies ainsi que l'impact du parall√©lisme dans l'algorithme. Le projet permet d‚Äôillustrer concr√®tement l‚Äôapport des m√©thodes al√©atoires pour l‚Äôalg√®bre lin√©aire num√©rique √† grande √©chelle.

---


